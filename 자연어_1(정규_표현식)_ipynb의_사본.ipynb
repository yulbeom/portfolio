{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yulbeom/portfolio/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4_1(%EC%A0%95%EA%B7%9C_%ED%91%9C%ED%98%84%EC%8B%9D)_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKYkHQdt7i10"
      },
      "source": [
        "# 정규 표현식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsAcqjj97WUb"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohOC-ezK7fV-"
      },
      "outputs": [],
      "source": [
        "text = 'I was wondering if anyone out there could enlighten me on this car'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vMXr6a68BgE"
      },
      "outputs": [],
      "source": [
        "r = re.compile('ab.') # .은 ab로 시작되는 어떤 것"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKzn88cB8UsM",
        "outputId": "51257f80-50ab-4c4e-e8a6-eec32248790f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(10, 13), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ],
      "source": [
        "r.search('kkkkkkkkkkabccccccccccc') # ab로 시작하는 거 찾음 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPLM9jG88qy_",
        "outputId": "7982eab6-9053-4d98-9136-cdfe89c769c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'was',\n",
              " 'wondering',\n",
              " 'if',\n",
              " 'anyone',\n",
              " 'out',\n",
              " 'there',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'me',\n",
              " 'on',\n",
              " 'this',\n",
              " 'car']"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "text.split(' ') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d0_M9jV8yd1"
      },
      "outputs": [],
      "source": [
        "r.match('kabc') # 없으면 안나옴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6nPWftM86dE",
        "outputId": "6ab29363-24ae-4c90-862b-246125a3059a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ],
      "source": [
        "r.match('abck') # 앞에 ab가 무조건 있어야 찾아진다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ2QThMB9FNo"
      },
      "outputs": [],
      "source": [
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b') # 길이가 1에서 2인 단어는 삭제, 노이즈 데이터를 삭제하는 방법으로 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz1z_Iuv9kaM",
        "outputId": "3de375c8-bdbe-46e0-d3c6-65df7a6be235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car\n"
          ]
        }
      ],
      "source": [
        "print(shortword.sub(\"\", text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEahsmimEEqL",
        "outputId": "d2fce77e-2593-45d8-f195-928a12dc5c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtacCERk9qe4"
      },
      "outputs": [],
      "source": [
        " from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cllEd5PBC3pI",
        "outputId": "a9399e82-5db9-477a-83b2-dea25731c67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전 :  ['have', 'going', 'love', 'lives', 'dies', 'watched', 'has']\n",
            "표제어 추출 후: ['have', 'going', 'love', 'life', 'dy', 'watched', 'ha']\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "words = ['have', 'going', 'love', 'lives', 'dies', 'watched', 'has']\n",
        "print('표제어 추출 전 : ', words)\n",
        "print('표제어 추출 후:',[lemmatizer.lemmatize(word) for word in words]) # 완벽하지 않다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CrCJ_H0bDhzU",
        "outputId": "47bf48df-fc78-4224-ca98-f68564018c5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'die'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "lemmatizer.lemmatize('dies', 'v') # 동사원형 -> 정확한 품사 정해줘야 된다, v - 동사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hrU90p6hE9FU",
        "outputId": "0236669e-fb72-47b7-f6ac-2cb8299aa8df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ha'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 195
        }
      ],
      "source": [
        "lemmatizer.lemmatize('has', 'n') # n - 명사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hohbcL3jFaVo"
      },
      "outputs": [],
      "source": [
        "sentence = \"This was not the map we found in Bill Bones's chest, but an accurate copy, complete in all thinsg--names and heights and soundings--with the single exception of the red crosses and the written notes\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2iuREaCL51w"
      },
      "source": [
        "# PorterStemmer 와 LancasterStemmer 차이"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9EUHPzxIXzO"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO6G3AkVIrO8"
      },
      "outputs": [],
      "source": [
        "p_stemmer = PorterStemmer()\n",
        "l_stemmer = LancasterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CSc42qeIwpb"
      },
      "outputs": [],
      "source": [
        "tokenized_sentence = word_tokenize(sentence) # 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qToakmdkI8A2",
        "outputId": "de791bf7-3005-4074-f191-423974447c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Bill', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'thinsg', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes']\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxjkwLqfJMre",
        "outputId": "55d7fb8c-da8b-4762-b634-6baaee013d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'bill', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thinsg', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note']\n",
            "['thi', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'bil', 'bon', \"'s\", 'chest', ',', 'but', 'an', 'acc', 'cop', ',', 'complet', 'in', 'al', 'thinsg', '--', 'nam', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'exceiv', 'of', 'the', 'red', 'cross', 'and', 'the', 'writ', 'not']\n"
          ]
        }
      ],
      "source": [
        "print([p_stemmer.stem(word) for word in tokenized_sentence]) # 어간 추출(Porter)\n",
        "print([l_stemmer.stem(word) for word in tokenized_sentence]) #  Lancaster "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K9BAoblntHb",
        "outputId": "2cc30afa-5635-4e83-d2a0-3ef02b5f85f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgRQTkgKJmrr"
      },
      "outputs": [],
      "source": [
        " from nltk.corpus import stopwords\n",
        " from nltk.tokenize import word_tokenize\n",
        " from konlpy.tag import Okt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCC-VxWQKWU-",
        "outputId": "4c38e038-c834-4604-cbee-004dcd2ae79e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ],
      "source": [
        "stop_words_list = stopwords.words('english')\n",
        "print(len(stop_words_list))\n",
        "print(stop_words_list[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDj86949nnFs"
      },
      "outputs": [],
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfdRch0or7hS",
        "outputId": "a631459c-fe70-49c9-f208-db258d53ae8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'yours', \"couldn't\", 'you', 'weren', 'yourselves', \"wouldn't\", 'in', 'itself', 'hasn', 'for', 'before', 'those', 'aren', 'nor', 'we', 'who', 'been', 'should', 'all', 's', \"wasn't\", \"doesn't\", 'can', 'under', 'below', 'did', 'is', 'wouldn', 'haven', 'during', 'or', 'myself', 'as', 'an', 'themselves', 'by', 'against', 'this', 'him', 'between', 'very', 'such', 'had', 'some', 're', 'ain', 'couldn', 'isn', 'wasn', 'over', 'only', 'of', 'then', 'yourself', 'again', 'their', 'them', \"it's\", 'they', 'not', 'needn', 'don', 'up', \"you'll\", 'same', 'off', \"didn't\", \"shouldn't\", 'with', 'was', \"she's\", 'after', 'ourselves', 'there', 'through', 'just', 'more', 'here', 'until', \"won't\", 'my', \"haven't\", 'our', 'd', 'i', 'own', \"shan't\", 'it', 'doing', 'no', \"weren't\", 'what', 'theirs', 'about', 'himself', 'both', 'o', 'm', \"hadn't\", 'do', 've', 'from', 'to', 'y', 'her', 'down', 'other', 'doesn', \"hasn't\", 'few', 'didn', 'if', 'me', 'but', 'a', 'its', 'on', 'any', \"you'd\", 'have', 'has', \"mustn't\", 'hadn', 'at', 'which', \"you've\", 'than', 'being', 't', 'out', 'will', 'further', 'because', 'above', \"isn't\", 'shouldn', 'while', 'where', 'that', 'most', 'once', \"should've\", 'too', 'he', 'now', \"that'll\", 'ours', \"you're\", 'his', 'hers', 'were', 'having', 'shan', 'does', 'so', \"don't\", 'the', 'll', 'why', 'and', 'how', \"aren't\", \"mightn't\", 'be', 'mustn', 'whom', 'won', 'into', 'these', 'am', 'she', 'when', 'each', 'your', 'mightn', 'are', 'herself', 'ma', \"needn't\"}\n"
          ]
        }
      ],
      "source": [
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqJgZ4Cxr_JP",
        "outputId": "2ed9f942-97c1-408b-97bd-868d2830b854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ],
      "source": [
        "print(word_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYRhBLqRpQed"
      },
      "outputs": [],
      "source": [
        "result = []\n",
        "for word in word_tokens:\n",
        "  if word not in stop_words:\n",
        "    result.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIy7SfOnpSuk",
        "outputId": "0a647ddb-54c5-4006-ccf5-e59af7be0edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlVbc2dGpkkY",
        "outputId": "4ff127cf-d5ff-4b2e-f65b-73f58463e1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 :  ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후 :  ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ],
      "source": [
        "okt = Okt()\n",
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "stop_words = set(stop_words.split(' '))\n",
        "word_tokens = okt.morphs(example)\n",
        "result = [word for word in word_tokens if not word in stop_words]\n",
        "print('불용어 제거 전 : ', word_tokens)\n",
        "print('불용어 제거 후 : ', result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHew2uYO2Zcc"
      },
      "source": [
        "# 정규표현식 실습\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHQf1Lg-2dLs",
        "outputId": "99c573a1-0805-4252-8819-99c2109f8d76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ],
      "source": [
        "import re\n",
        "r = re.compile(\"a.c\") # a()c가 있는지 확인, New Line을 제외한 모든 글자 ex).n -> an apple, on the tree \n",
        "r.search(\"kkk\")\n",
        "r.search(\"abc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F4ljfWJ2bVF",
        "outputId": "c8dcc7fd-0232-4782-e0ea-a088731e623a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "r = re.compile(\"ab?c\") # 앞의 문자나 부분식을 0개나 1개 찾습니다.ex) te?n -> ten, tn,teen(x) \n",
        "r.search(\"abbc\")\n",
        "r.search(\"abc\")\n",
        "r.search(\"ac\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BnljPAOr67G",
        "outputId": "b1d5f1eb-0d6e-4d40-acf3-46e84f8c339b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 6), match='abbbbc'>"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ],
      "source": [
        "r = re.compile(\"ab*c\") # 앞의 문자나 부분식을 0개 이상 찾습니다. ex) ab* -> ab, aabb, abb, aaaa / ab*d -> ad, abd, abbbbd \n",
        "r.search(\"a\")\n",
        "r.search(\"ac\")\n",
        "r.search(\"abc\")\n",
        "r.search(\"abbbbc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX-dCk0O3qhS",
        "outputId": "bd658477-2213-430d-f434-608c4ad13532"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 6), match='abbbbc'>"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ],
      "source": [
        "r = re.compile(\"ab+c\") # 앞의 문자나 부분식을 1개 이상 찾습니다. ex) zo+ -> zo, zoo \n",
        "r.search(\"ac\")\n",
        "r.search(\"abc\")\n",
        "r.search(\"abbbbc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyHu0ENJ463N",
        "outputId": "3297ab19-420c-4510-ea5a-bd0772b8dbcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ab'>"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ],
      "source": [
        "r = re.compile(\"^ab\") # 입력 문자열의 시작 부분에서 위치를 찾습니다. ex)  ^abc -> abcdef / ^a?bc -> bcdef, abcdef\n",
        "r.search(\"bbc\")\n",
        "r.search(\"zab\")\n",
        "r.search(\"abz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQQMiEpt5GOS",
        "outputId": "7812da09-a8f7-408e-92fb-32540c1e2ba3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='abbc'>"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ],
      "source": [
        "r = re.compile(\"ab{2}c\") # n의 수만큼 정확하게 앞글자를 반복합니다. ex) te{2}n -> teen\n",
        "r.search(\"ac\")\n",
        "r.search(\"abc\")\n",
        "r.search(\"abbbbbc\")\n",
        "r.search(\"abbc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO5AgB3r5aoX",
        "outputId": "628273e1-bdee-4e1a-81e5-026b32bbf1aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 10), match='abbbbbbbbc'>"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ],
      "source": [
        "r = re.compile(\"ab{2,8}c\") # {a, b} a에서 b까지 ex) te{1,2}n -> ten, teen \n",
        "r.search(\"ac\")\n",
        "r.search(\"abc\") # b 1개\n",
        "r.search(\"abbbbbbbbbc\") # b 9개\n",
        "r.search(\"abbc\") # b 2개\n",
        "r.search(\"abbbbbbbbc\") # b 8개"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G60WrwWQ5_0v",
        "outputId": "13da9453-bdeb-43b4-e2c6-844dc3e181e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='b'>"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ],
      "source": [
        "r = re.compile(\"[abc]\") # 문자 집합, 괄호로 묶인 문자 중 하나를 찾습니다. ex) [abc] -> plain \n",
        "r.search(\"zzz\")\n",
        "r.search(\"a\")\n",
        "r.search(\"aaaaaaa\")\n",
        "r.search(\"baac\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmAOQAN07Z11",
        "outputId": "3f531d6f-2135-471a-ce2d-be4dad8dba15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ],
      "source": [
        "r = re.compile(\"[a-z]\") # [a-zA-Z], [0-9]\n",
        "r.search(\"AAA\")\n",
        "r.search(\"111\")\n",
        "r.search(\"aBC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOMVLOLm75DU",
        "outputId": "18440f99-e869-422e-e268-c1d952fdfb16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='1'>"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ],
      "source": [
        "r = re.compile(\"[^abc]\") # 음의 범위 문자. 지정한 범위에서 있지 않은 문자를 찾습니다.ex) [^a-z] -> KNOW know  \n",
        "r.search(\"a\")\n",
        "r.search(\"ab\")\n",
        "r.search(\"b\")\n",
        "r.search(\"d\")\n",
        "r.search(\"1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i4fz5tw70sR"
      },
      "source": [
        "# r.match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niPOwvG773k8",
        "outputId": "53ce0fab-fcc4-4c9e-b2d8-d66b82887b3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ],
      "source": [
        "r = re.compile(\"ab.\")\n",
        "r.match(\"nabc\")\n",
        "r.match(\"nabc\")\n",
        "r.match(\"abcn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzb-pPZE7lNO",
        "outputId": "be7b005a-dfdf-429b-d28d-ec7c5b561797"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ],
      "source": [
        "text = \"\"\"이름 : 김철수\n",
        "전화번호 : 010 - 1234 - 1234\n",
        "나이 : 30\n",
        "성별 : 남\"\"\"\n",
        "#text에서 숫자만 추출하려면?\n",
        "re.findall(\"\\d+\", text)  # 숫자만 출력\n",
        "re.findall(\"\\d+\",\"자연어처리\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faLczCYD9I4D",
        "outputId": "dc9d633d-34a0-48ce-9293-14f508eec9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RegularexpressionAregularexpressionregexorregexpsometimescalledarationalexpressionisintheoreticalcomputerscienceandformallanguagetheoryasequenceofcharactersthatdefineasearchpattern\n"
          ]
        }
      ],
      "source": [
        "text = \"Regular expression : A regular expression, regex or regexp[1](sometimes called a rational expression)[2][3]is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"# 아무 영어 문장 사용\n",
        "p_text = re.sub('[^a-zA-Z]', '', text) # re.sub(a, b, text) text에서 a형식을 찾아서 b로 바꿔라\n",
        "print(p_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ-Hd16f-UON"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"100 John PROF\n",
        "101 James STUD\n",
        "102 Mac STUD\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0flMVcJAEHAB",
        "outputId": "9087bd54-193e-480d-a8af-f5ccb975b997"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ],
      "source": [
        "# 공백 기준 분리\n",
        "re.split('\\s+',text) # s : space , + : 하나 이상 그러므로 s+ 한칸이상의 여백이라는 뜻 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1O2RekwEhi1",
        "outputId": "84395389-5b21-4e2c-afa4-0121dfe54acd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', '101', '102']"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ],
      "source": [
        "# 숫자만 추출\n",
        "re.findall('\\d+', text) # d : 숫자 , d+ : 하나이상의 숫자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rajRUl3_FP3E",
        "outputId": "5fa4e95e-1321-4772-fe4f-f392405cb671"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PROF', 'STUD', 'STUD']"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ],
      "source": [
        "# 텍스트가 대문자인 행 추출 \n",
        "re.findall('[A-Z]', text)\n",
        "re.findall('[A-Z]{4}', text) # 대문자가 연속 4번 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eLHt8LVFgd7",
        "outputId": "7209df05-1aa3-4cc6-d814-823101b133b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['John', 'James', 'Mac']"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ],
      "source": [
        "# 처음만 대문자 추출\n",
        "re.findall('[A-Z][a-z]+', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrX3BBnoGfp8"
      },
      "source": [
        "# RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNC_O2zmGisW"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "text = \"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7evDd9BGRD2",
        "outputId": "d27b2ab5-550b-4288-da9d-dba9c831342e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ],
      "source": [
        "# 특수문자 혹은 공백 기준 단어 및 숫자 분리\n",
        "tokenizer1 = RegexpTokenizer(\"[\\w]+\")\n",
        "print(tokenizer1.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcLYPyu0GfGw",
        "outputId": "263781b6-93c1-47c1-ca3e-e673bcb2746a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', \"Mr.Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ],
      "source": [
        "# 공백 기준 단어 분리\n",
        "tokenizer2 = RegexpTokenizer(\"\\s+\", gaps=True) # gaps = True 안넣고 돌려보고 비교해보기 \n",
        "print(tokenizer2.tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4P-Fd9KIqzs"
      },
      "source": [
        "# 텍스트 레이블 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R43UMiFZIt64"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwWeSs7tHeSp",
        "outputId": "5d454a90-ff1d-41f3-ec70-ed2472789d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ],
      "source": [
        "# 문장 토큰화 (. 마침표 기준으로 토큰화)\n",
        "sentences = sent_tokenize(raw_text)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iizEysUyJUU1",
        "outputId": "d5436832-30af-40db-f211-2ef72afc45be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'barber', 'is', 'a', 'person', '.', 'a', 'barber', 'is', 'good', 'person', '.', 'a', 'barber', 'is', 'huge', 'person', '.', 'he', 'Knew', 'A', 'Secret', '!', 'The', 'Secret', 'He', 'Kept', 'is', 'huge', 'secret', '.', 'Huge', 'secret', '.', 'His', 'barber', 'kept', 'his', 'word', '.', 'a', 'barber', 'kept', 'his', 'word', '.', 'His', 'barber', 'kept', 'his', 'secret', '.', 'But', 'keeping', 'and', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'the', 'barber', 'crazy', '.', 'the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.']\n"
          ]
        }
      ],
      "source": [
        "# 단어 토큰화\n",
        "word = word_tokenize(raw_text)\n",
        "print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhtgA4swK3QP",
        "outputId": "923b760d-b528-49d8-ebe8-36877b32d930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word :  a\n",
            "word :  barber\n",
            "result :  ['barber']\n",
            "{'barber': 1}\n",
            "word :  is\n",
            "word :  a\n",
            "word :  person\n",
            "result :  ['barber', 'person']\n",
            "{'barber': 1, 'person': 1}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person']]\n",
            "word :  a\n",
            "word :  barber\n",
            "result :  ['barber']\n",
            "{'barber': 2, 'person': 1}\n",
            "word :  is\n",
            "word :  good\n",
            "result :  ['barber', 'good']\n",
            "{'barber': 2, 'person': 1, 'good': 1}\n",
            "word :  person\n",
            "result :  ['barber', 'good', 'person']\n",
            "{'barber': 2, 'person': 2, 'good': 1}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person']]\n",
            "word :  a\n",
            "word :  barber\n",
            "result :  ['barber']\n",
            "{'barber': 3, 'person': 2, 'good': 1}\n",
            "word :  is\n",
            "word :  huge\n",
            "result :  ['barber', 'huge']\n",
            "{'barber': 3, 'person': 2, 'good': 1, 'huge': 1}\n",
            "word :  person\n",
            "result :  ['barber', 'huge', 'person']\n",
            "{'barber': 3, 'person': 3, 'good': 1, 'huge': 1}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person']]\n",
            "word :  he\n",
            "word :  knew\n",
            "result :  ['knew']\n",
            "{'barber': 3, 'person': 3, 'good': 1, 'huge': 1, 'knew': 1}\n",
            "word :  a\n",
            "word :  secret\n",
            "result :  ['knew', 'secret']\n",
            "{'barber': 3, 'person': 3, 'good': 1, 'huge': 1, 'knew': 1, 'secret': 1}\n",
            "word :  !\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret']]\n",
            "word :  the\n",
            "word :  secret\n",
            "result :  ['secret']\n",
            "{'barber': 3, 'person': 3, 'good': 1, 'huge': 1, 'knew': 1, 'secret': 2}\n",
            "word :  he\n",
            "word :  kept\n",
            "result :  ['secret', 'kept']\n",
            "{'barber': 3, 'person': 3, 'good': 1, 'huge': 1, 'knew': 1, 'secret': 2, 'kept': 1}\n",
            "word :  is\n",
            "word :  huge\n",
            "result :  ['secret', 'kept', 'huge']\n",
            "{'barber': 3, 'person': 3, 'good': 1, 'huge': 2, 'knew': 1, 'secret': 2, 'kept': 1}\n",
            "word :  secret\n",
            "result :  ['secret', 'kept', 'huge', 'secret']\n",
            "{'barber': 3, 'person': 3, 'good': 1, 'huge': 2, 'knew': 1, 'secret': 3, 'kept': 1}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret']]\n",
            "word :  huge\n",
            "result :  ['huge']\n",
            "{'barber': 3, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 3, 'kept': 1}\n",
            "word :  secret\n",
            "result :  ['huge', 'secret']\n",
            "{'barber': 3, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 4, 'kept': 1}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret']]\n",
            "word :  his\n",
            "word :  barber\n",
            "result :  ['barber']\n",
            "{'barber': 4, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 4, 'kept': 1}\n",
            "word :  kept\n",
            "result :  ['barber', 'kept']\n",
            "{'barber': 4, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 4, 'kept': 2}\n",
            "word :  his\n",
            "word :  word\n",
            "result :  ['barber', 'kept', 'word']\n",
            "{'barber': 4, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 4, 'kept': 2, 'word': 1}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word']]\n",
            "word :  a\n",
            "word :  barber\n",
            "result :  ['barber']\n",
            "{'barber': 5, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 4, 'kept': 2, 'word': 1}\n",
            "word :  kept\n",
            "result :  ['barber', 'kept']\n",
            "{'barber': 5, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 4, 'kept': 3, 'word': 1}\n",
            "word :  his\n",
            "word :  word\n",
            "result :  ['barber', 'kept', 'word']\n",
            "{'barber': 5, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 4, 'kept': 3, 'word': 2}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word']]\n",
            "word :  his\n",
            "word :  barber\n",
            "result :  ['barber']\n",
            "{'barber': 6, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 4, 'kept': 3, 'word': 2}\n",
            "word :  kept\n",
            "result :  ['barber', 'kept']\n",
            "{'barber': 6, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 4, 'kept': 4, 'word': 2}\n",
            "word :  his\n",
            "word :  secret\n",
            "result :  ['barber', 'kept', 'secret']\n",
            "{'barber': 6, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 5, 'kept': 4, 'word': 2}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret']]\n",
            "word :  but\n",
            "word :  keeping\n",
            "result :  ['keeping']\n",
            "{'barber': 6, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 5, 'kept': 4, 'word': 2, 'keeping': 1}\n",
            "word :  and\n",
            "word :  keeping\n",
            "result :  ['keeping', 'keeping']\n",
            "{'barber': 6, 'person': 3, 'good': 1, 'huge': 3, 'knew': 1, 'secret': 5, 'kept': 4, 'word': 2, 'keeping': 2}\n",
            "word :  such\n",
            "word :  a\n",
            "word :  huge\n",
            "result :  ['keeping', 'keeping', 'huge']\n",
            "{'barber': 6, 'person': 3, 'good': 1, 'huge': 4, 'knew': 1, 'secret': 5, 'kept': 4, 'word': 2, 'keeping': 2}\n",
            "word :  secret\n",
            "result :  ['keeping', 'keeping', 'huge', 'secret']\n",
            "{'barber': 6, 'person': 3, 'good': 1, 'huge': 4, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2}\n",
            "word :  to\n",
            "word :  himself\n",
            "word :  was\n",
            "word :  driving\n",
            "result :  ['keeping', 'keeping', 'huge', 'secret', 'driving']\n",
            "{'barber': 6, 'person': 3, 'good': 1, 'huge': 4, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1}\n",
            "word :  the\n",
            "word :  barber\n",
            "result :  ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber']\n",
            "{'barber': 7, 'person': 3, 'good': 1, 'huge': 4, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1}\n",
            "word :  crazy\n",
            "result :  ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy']\n",
            "{'barber': 7, 'person': 3, 'good': 1, 'huge': 4, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy']]\n",
            "word :  the\n",
            "word :  barber\n",
            "result :  ['barber']\n",
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 4, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1}\n",
            "word :  went\n",
            "result :  ['barber', 'went']\n",
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 4, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1}\n",
            "word :  up\n",
            "word :  a\n",
            "word :  huge\n",
            "result :  ['barber', 'went', 'huge']\n",
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1}\n",
            "word :  mountain\n",
            "result :  ['barber', 'went', 'huge', 'mountain']\n",
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
            "word :  .\n",
            "preprocessed :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_words = set(stopwords.words('english')) # stopwords 불용어\n",
        "for sentence in sentences:\n",
        "  # 단어 토큰화\n",
        "  tokenized_sentence = word_tokenize(sentence)\n",
        "  result=[]\n",
        "  \n",
        "  for word in tokenized_sentence:\n",
        "    word = word.lower()\n",
        "    if word not in stop_words: # word가 불용어가 아니면\n",
        "      if len(word)>2:          # word 길이가 3이상이면\n",
        "        result.append(word)    # result 리스트에 넣는다\n",
        "        \n",
        "        if word not in vocab:    # word가 vocab 딕셔너리안에 없다면\n",
        "          vocab[word] = 0        # word : 0 으로 vocab 딕셔너리안에 입력\n",
        "        vocab[word] += 1  \n",
        "  preprocessed_sentences.append(result)\n",
        "  print('preprocessed : ', preprocessed_sentences)\n",
        "print(preprocessed_sentences)\n",
        "print(vocab)\n",
        "print(vocab['barber'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxfy0csvMqWQ",
        "outputId": "5bcc6b23-10d1-4854-826f-3c4ed1e19cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n",
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ],
      "source": [
        "# vocab 딕셔너리 빈도수가 높은 순 정렬\n",
        "vocab_sorted = sorted(vocab.items(), key=lambda x:x[1], reverse = True) # 여기서 key는 딕셔너리키가 아님 sorted()함수의 매개변수\n",
        "print(vocab_sorted)\n",
        "# 높은 빈도수일수록 낮은 정수를 부여(정수는 1부터)\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab_sorted:\n",
        "  if frequency>1:\n",
        "    i += 1\n",
        "    word_to_index[word] = i\n",
        "print(word_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biL61Z_ztwss",
        "outputId": "80eab475-6685-4079-f4b3-41be5a055db7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n",
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 5\n",
        "\n",
        "# 인덱스가 5초과인 단어 제거\n",
        "words_frequency = [word for word, index in word_to_index.items() if index > vocab_size]\n",
        "\n",
        "# 해당 단어에 대한 인덱스 정보를 삭제\n",
        "for w in words_frequency:\n",
        "  del word_to_index[w]\n",
        "print(word_to_index)\n",
        "\n",
        "# 새로운 단어가 들어왔을때 인덱싱 처리를 어떻게 할건가\n",
        "word_to_index['OOV'] = len(word_to_index) + 1  # Out Of Vocabulary\n",
        "print(word_to_index) \n",
        "# 단어 집합에 있는 단어라면 해당 단어의 숫자를 없으면 'OOV'의 숫자를 반환\n",
        "encoded_sentences = []\n",
        "for sentence in preprocessed_sentences:\n",
        "    encoded_sentence = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            encoded_sentence.append(word_to_index[word])\n",
        "        except KeyError:\n",
        "            encoded_sentence.append(word_to_index['OOV'])\n",
        "    encoded_sentences.append(encoded_sentence)\n",
        "print(encoded_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 인코딩을 좀 더 쉽게 하려면? -> Counter 모듈 사용\n",
        "from collections import Counter\n",
        "print(preprocessed_sentences)\n",
        "\n",
        "all_words_list = sum(preprocessed_sentences, []) # preprocessed_sentences -> 이중리스트 불필요하므로, 하나의 리스트에 들어가게끔 한 처리\n",
        "print(all_words_list)\n",
        "\n",
        "vocab = Counter(all_words_list)\n",
        "vocab\n",
        "vocab['barber']\n",
        "\n",
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # Counter 클래스안에 있는 most_common 메소드\n",
        "vocab\n",
        "\n",
        "# 높은 빈도수 순 인덱스 부여\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab :\n",
        "    i += 1\n",
        "    word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6B5EC8xF906",
        "outputId": "aa30106e-fc8a-450f-eeca-10d94c850388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n",
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### FreqDist 모듈\n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "\n",
        "vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "print(vocab)\n",
        "\n",
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpuC-9GLGBpq",
        "outputId": "5e80e827-9b4a-49db-cf8f-74c9017a2661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n",
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### tensorflow.keras 모듈\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "print(tokenizer.word_index) # word : index\n",
        "print(tokenizer.word_counts) # word : counts(빈도 수)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences)) # fit_on_texts 하고 난 후 인덱싱 후 결과???\n",
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words=vocab_size+1) # 상위 5개 단어만 사용 , num_words 은 parameter, +1은 왜???\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnhQ1qauGBzF",
        "outputId": "731c76b0-43b9-4f9b-b0cb-836232effa49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size+2, oov_token = 'OOV') # +2하는 이유는 OOV 때문\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "print(tokenizer.word_index['OOV'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK2JeuSQHQQK",
        "outputId": "8da84d3e-3813-4b26-97c2-d73b4e6692de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "자연어 1(정규 표현식).ipynb의 사본",
      "provenance": [],
      "authorship_tag": "ABX9TyOG1yqZHQoO5Wc6hQ2/CVFi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}